Automatic model selection algorithms have the potential to assist economic researchers a great deal. As the big data revolution starts to play a more integral role in economic research, algorithms like Autometrics, the Lasso and BSTS will become more mainstream. The results of this thesis demonstrate that while the three algorithms have broadly the same goal of selecting a sparse model from a much more general model, the properties of each algorithm vary. Of particular relevance to the results is the correlation structure of the DGP. Because empirically the structure of the DGP is impossible to know it should be viewed unfavourably if an algorithm's properties depend on a particular correlation structure.

This thesis began by first outlining the theory behind the three different algorithms studied. Then, results from simulations conducted across three different correlation structures were presented individually. Within each of these three sets of simulations, different non-centralities for the relevant variable were tested. Also examined was the impact of including more variables than observations in the initial GUM. Algorithms were compared to each other and against a benchmark to examine the costs of search and costs of inference. Finally the results across different correlation structures were analyzed.   

Of the three algorithms studied, Autometrics was found to be by the far the most consistent across correlation structures. The Lasso's results vary a great deal depending on the correlation structure and the non-centralities  of the relevant variables. This makes it difficult to describe the general properties of a model the Lasso selects. BSTS selects very sparse models overall, with the properties of the selected model dependent on the correlation structure as well. The number of candidate variables did not seem to matter in any of the algorithms; that is in most simulations, each algorithm was just as effective when the number of variables exceeded the number of observations. 

A nowcasting application of automatic model selection was then considered, where each of the algorithms was used to nowcast the incidence of the flu in the United States, using information from Google search queries. All three algorithms generated very good in-sample nowcasts. The out-of-sample nowcasts were also very accurate. Autometrics and the Lasso were also used to try to forecast the flu, with success. The results were compared to actual Google Flu Trends estimates, and were shown to be even more accurate. The results in this section suggest that nowcasting and forecasting may be a profitable application of automatic model selection algorithms. However given that the goal of nowcasting is to minimize predictive error, more work needs to be done to understand the best way to use automatic model selection in the nowcasting arena.  

The research presented in this thesis has demonstrated that if economists wish is to effectively and appropriately take advantage of the new tools for big data like the ones presented here, it is essential they do their research. The results demonstrate that not all algorithms are created equally, and that choosing one technique over another can have a significant impact on results. That said, the results in this thesis suggest that Autometrics is a promising, consistent and reliable algorithm and has the potential to greatly simplify the work of empirical researchers. 